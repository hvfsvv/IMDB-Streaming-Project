{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "babe5a5c-5a5f-4010-a607-7f907ef4c8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Final Project\n",
    "##By Sarah SHAHIN, Najlaa ALLIOUI, Hafsa REDOUANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "96d0a76e-8d65-4947-9237-99ab95289d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4e9975-718e-4441-afc2-46bd85428bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# --- CELL 1: IMPORTS, CONFIGURATION, LOADING, AND UNIVERSAL CLEANING ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8f0dc1-93d4-462c-b36b-5bcc077fb0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CELL 1: IMPORTS, CONFIGURATION, LOADING, AND UNIVERSAL CLEANING ---\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, LongType \n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql.utils import AnalysisException \n",
    "\n",
    "# Initialize SparkSession \n",
    "spark = SparkSession.builder.appName(\"IMDB_Final_Project\").getOrCreate()\n",
    "\n",
    "\n",
    "NA_VALUE = r'\\N'\n",
    "CURRENT_YEAR = 2025 \n",
    "\n",
    "\n",
    "UC_VOLUME_NAME = \"raw_imdb_files\" \n",
    "\n",
    "\n",
    "STORAGE_ROOT = f\"dbfs:/Volumes/workspace/imdb_project/{UC_VOLUME_NAME}/\" \n",
    "\n",
    "\n",
    "DBFS_PATH = STORAGE_ROOT \n",
    "\n",
    "\n",
    "# --- IMDB File Dictionary ---\n",
    "DATA_FILES = {\n",
    "    'name_basics': DBFS_PATH + \"name.basics.tsv\",\n",
    "    'title_basics': DBFS_PATH + \"title.basics.tsv\",\n",
    "    'title_ratings': DBFS_PATH + \"title.ratings.tsv\",\n",
    "    'title_crew': DBFS_PATH + \"title.crew.tsv\",\n",
    "    'title_akas': DBFS_PATH + \"title.akas.tsv\",\n",
    "    'title_episode': DBFS_PATH + \"title.episode.tsv\",\n",
    "    'title_principals': DBFS_PATH + \"title.principals.tsv\"\n",
    "}\n",
    "\n",
    "# --- CORE LOADING FUNCTION ---\n",
    "def load_and_clean_imdb_data(file_name, file_path, schema_override=None):\n",
    "    print(f\"Loading {file_name}...\")\n",
    "    try:\n",
    "        df = spark.read.csv(\n",
    "            file_path,\n",
    "            sep='\\t',\n",
    "            header=True,\n",
    "            nullValue=NA_VALUE,\n",
    "            inferSchema=(schema_override is None), \n",
    "            schema=schema_override\n",
    "        )\n",
    "        count = df.count()\n",
    "        print(f\"Success! {file_name} loaded. {count:,} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\" SPARK LOADING FAILED for {file_name}. (Error: {e})\")\n",
    "        return None\n",
    "\n",
    "# --- SAFE CAST FUNCTION (Compatible with older Spark, handles '') ---\n",
    "def safe_cast_pyspark(col_name, target_type, pattern=r'[^\\d]'):\n",
    "    \"\"\"Replaces non-matching characters, converts resulting '' to NULL, then casts.\"\"\"\n",
    "    cleaned_col = regexp_replace(col(col_name), pattern, '')\n",
    "    return F.when(\n",
    "        (col(col_name).isNull()) | (col(col_name) == F.lit(\"\")) | (cleaned_col == F.lit(\"\")),\n",
    "        F.lit(None)\n",
    "    ).otherwise(\n",
    "        cleaned_col.cast(target_type)\n",
    "    )\n",
    "\n",
    "# --- LOADING AND OVERRIDE STRATEGY ---\n",
    "print(\"\\n--- Starting Data Load and Cleaning ---\")\n",
    "\n",
    "# A. Load Non-Critical DataFrames\n",
    "df_crew = load_and_clean_imdb_data('title.crew', DATA_FILES['title_crew']) \n",
    "df_akas = load_and_clean_imdb_data('title.akas', DATA_FILES['title_akas'])\n",
    "df_episode = load_and_clean_imdb_data('title.episode', DATA_FILES['title_episode'])\n",
    "\n",
    "# CRITICAL FIX: Rename titleId to tconst for consistency\n",
    "if df_akas:\n",
    "    if 'titleId' in df_akas.columns:\n",
    "        df_akas = df_akas.withColumnRenamed(\"titleId\", \"tconst\")\n",
    "        print(\"df_akas: Renamed 'titleId' to 'tconst'.\")\n",
    "    elif 'tconst' not in df_akas.columns:\n",
    "        print(\"df_akas: Could not find 'titleId' or 'tconst'. Check schema.\")\n",
    "\n",
    "\n",
    "# B. df_basics (Cleaning startYear/runtimeMinutes)\n",
    "df_basics = spark.read.csv(DATA_FILES['title_basics'], sep='\\t', header=True, nullValue=NA_VALUE, inferSchema=False)\n",
    "df_basics = df_basics.withColumn(\n",
    "    \"runtimeMinutes\",\n",
    "    safe_cast_pyspark(\"runtimeMinutes\", IntegerType())\n",
    ").withColumn(\n",
    "    \"startYear\",\n",
    "    safe_cast_pyspark(\"startYear\", IntegerType())\n",
    ")\n",
    "print(f\"title.basics loaded and cleaned. {df_basics.count():,} rows.\")\n",
    "\n",
    "# C. df_names (birthYear/deathYear override)\n",
    "df_names = spark.read.csv(DATA_FILES['name_basics'], sep='\\t', header=True, nullValue=NA_VALUE, inferSchema=False)\n",
    "df_names = df_names.select(\"nconst\", \"primaryName\", \"birthYear\", \"deathYear\", \"primaryProfession\", \"knownForTitles\")\n",
    "df_names = df_names.withColumn(\n",
    "    \"birthYear\",\n",
    "    safe_cast_pyspark(\"birthYear\", IntegerType())\n",
    ").withColumn(\n",
    "    \"deathYear\",\n",
    "    safe_cast_pyspark(\"deathYear\", IntegerType())\n",
    ")\n",
    "print(f\"name.basics loaded and cleaned. {df_names.count():,} rows.\")\n",
    "\n",
    "# D. df_ratings (numVotes and averageRating)\n",
    "df_ratings = spark.read.csv(DATA_FILES['title_ratings'], sep='\\t', header=True, nullValue=NA_VALUE, inferSchema=False)\n",
    "df_ratings = df_ratings.withColumn(\n",
    "    \"numVotes\",\n",
    "    safe_cast_pyspark(\"numVotes\", LongType())\n",
    ").withColumn(\n",
    "    \"averageRating\",\n",
    "    safe_cast_pyspark(\"averageRating\", FloatType(), pattern=r'[^\\d\\.]')\n",
    ")\n",
    "df_ratings = df_ratings.filter(col(\"numVotes\").isNotNull() & (col(\"numVotes\") > 0))\n",
    "print(f\"title.ratings loaded and cleaned. {df_ratings.count():,} rows.\")\n",
    "\n",
    "# E. df_principals (ordering)\n",
    "df_principals = spark.read.csv(DATA_FILES['title_principals'], sep='\\t', header=True, nullValue=NA_VALUE, inferSchema=False)\n",
    "df_principals = df_principals.withColumn(\n",
    "    \"ordering\",\n",
    "    safe_cast_pyspark(\"ordering\", IntegerType())\n",
    ")\n",
    "print(f\"title.principals loaded and cleaned. {df_principals.count():,} rows.\")\n",
    "\n",
    "print(\"\\nDataFrame preparation complete. Proceed to analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee78acb4-2670-449f-80ce-9bdb66b0ae98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# --- CELL 2: BASIC STATISTICS (RUNTIME AND YEARS) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63097df0-83b3-412a-a959-21736f53fdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"--- A. PEOPLE ANALYSIS (df_names) ---\")\n",
    "\n",
    "# How many total people in data set?\n",
    "total_people = df_names.count()\n",
    "print(f\"Total people in the dataset: {total_people:,}\")\n",
    "\n",
    "# What is the earliest year of birth?\n",
    "earliest_year_row = df_names.agg(F.min(\"birthYear\").alias(\"min_year\")).collect()[0]\n",
    "earliest_year = earliest_year_row[\"min_year\"]\n",
    "print(f\"Earliest year of birth: {earliest_year}\")\n",
    "\n",
    "# How many years ago was this person born?\n",
    "if earliest_year:\n",
    "    age_ago = CURRENT_YEAR - earliest_year\n",
    "    print(f\"This person was born approximately {age_ago} years ago (based on {CURRENT_YEAR}).\")\n",
    "\n",
    "# Using only the data in the data set, determine if this date of birth correct.\n",
    "df_impossible_births = df_names.filter(\n",
    "    col(\"birthYear\").isNotNull() & \n",
    "    col(\"deathYear\").isNotNull() & \n",
    "    (col(\"birthYear\") > col(\"deathYear\"))\n",
    ")\n",
    "impossible_count = df_impossible_births.count()\n",
    "print(f\"Number of impossible records (Birth Year > Death Year): {impossible_count:,}\")\n",
    "\n",
    "# What is the most recent date of birth?\n",
    "most_recent_year_row = df_names.agg(F.max(\"birthYear\").alias(\"max_year\")).collect()[0]\n",
    "most_recent_year = most_recent_year_row[\"max_year\"]\n",
    "print(f\"Most recent year of birth: {most_recent_year}\")\n",
    "\n",
    "# What percentage of the people do not have a listed date of birth?\n",
    "people_with_birth_year = df_names.filter(col(\"birthYear\").isNotNull()).count()\n",
    "people_without_birth_year = total_people - people_with_birth_year\n",
    "percentage_without_birth = (people_without_birth_year / total_people) * 100\n",
    "print(f\"Percentage of people without a listed date of birth: {percentage_without_birth:.2f}%\")\n",
    "\n",
    "print(\"\\n--- B. TITLE ANALYSIS (df_basics) ---\")\n",
    "\n",
    "# Release Year Distribution (Top 10):\n",
    "print(\"\\nRelease Year Distribution (Top 10):\")\n",
    "df_release_years = df_basics.filter(\n",
    "    F.col(\"startYear\").isNotNull()\n",
    ").groupBy(\n",
    "    F.col(\"startYear\")\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"count\")\n",
    ").orderBy(\n",
    "    F.col(\"count\").desc()\n",
    ").limit(10)\n",
    "\n",
    "df_release_years.show()\n",
    "\n",
    "# What is the length of the longest \"short\" after 1900?\n",
    "longest_short_row = df_basics.filter(\n",
    "    (col(\"titleType\") == \"short\") & \n",
    "    (col(\"startYear\") >= 1900) & \n",
    "    col(\"runtimeMinutes\").isNotNull()\n",
    ").agg(F.max(\"runtimeMinutes\").alias(\"longest_runtime\")).collect()[0]\n",
    "longest_short = longest_short_row[\"longest_runtime\"]\n",
    "print(f\"Length of the longest 'short' after 1900 (in minutes): {longest_short}\")\n",
    "\n",
    "# What is the length of the shortest \"movie\" after 1900?\n",
    "shortest_movie_row = df_basics.filter(\n",
    "    (col(\"titleType\") == \"movie\") & \n",
    "    (col(\"startYear\") >= 1900) & \n",
    "    col(\"runtimeMinutes\").isNotNull()\n",
    ").agg(F.min(\"runtimeMinutes\").alias(\"shortest_runtime\")).collect()[0]\n",
    "shortest_movie = shortest_movie_row[\"shortest_runtime\"]\n",
    "print(f\"Length of the shortest 'movie' after 1900 (in minutes): {shortest_movie}\")\n",
    "\n",
    "\n",
    "# List of all of the genres represented.\n",
    "print(\"\\nList of all genres represented (Top 20 shown):\")\n",
    "df_basics.withColumn(\"genre\", F.explode(F.split(col(\"genres\"), \",\"))) \\\n",
    "         .select(col(\"genre\")).distinct().orderBy(\"genre\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5716bcf7-8599-4fb7-b0c4-547811331090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# --- CELL 3: BATCH ANALYSIS ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d08636-083e-4d4f-a91e-f4a63801df46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "MIN_VOTES_THRESHOLD = 10000 \n",
    "print(f\"Applying minimum vote threshold: {MIN_VOTES_THRESHOLD:,} votes.\")\n",
    "\n",
    "\n",
    "# 1. Join basics and ratings\n",
    "df_movie_data = df_basics.join(df_ratings, on=\"tconst\", how=\"inner\")\n",
    "\n",
    "# 2. Filter for 'movie', 'Comedy', AND minimum votes\n",
    "df_meaningful_comedy_movies = df_movie_data.filter(\n",
    "    (col(\"titleType\") == \"movie\") & \n",
    "    (col(\"genres\").contains(\"Comedy\")) &\n",
    "    (col(\"numVotes\") >= MIN_VOTES_THRESHOLD) \n",
    ")\n",
    "\n",
    "# 3. Order by rating (desc) then votes (desc) and select the top one\n",
    "highest_rated_comedy = df_meaningful_comedy_movies.orderBy(\n",
    "    col(\"averageRating\").desc(), \n",
    "    col(\"numVotes\").desc()\n",
    ").select(\"tconst\", \"primaryTitle\", \"averageRating\", \"numVotes\").limit(1)\n",
    "\n",
    "print(\"\\nHighest Rated Comedy 'Movie' (STATISTICALLY SIGNIFICANT):\")\n",
    "highest_rated_comedy.show(truncate=False)\n",
    "\n",
    "try:\n",
    "    best_movie_details = highest_rated_comedy.collect()[0]\n",
    "    best_movie_tconst = best_movie_details[\"tconst\"]\n",
    "    best_movie_title = best_movie_details[\"primaryTitle\"]\n",
    "    \n",
    "    print(f\"--- RESULTS FOR {best_movie_title} (tconst: {best_movie_tconst}) ---\")\n",
    "\n",
    "    # Who was the director of the movie?\n",
    "    print(f\"\\nDirector(s) of '{best_movie_title}':\")\n",
    "    df_directors = df_crew.filter(col(\"tconst\") == lit(best_movie_tconst)) \\\n",
    "                          .select(F.explode(F.split(col(\"directors\"), \",\")).alias(\"nconst\")) \\\n",
    "                          .join(df_names.select(\"nconst\", \"primaryName\"), on=\"nconst\", how=\"inner\")\n",
    "    \n",
    "    df_directors.select(col(\"primaryName\").alias(\"Director Name\")).distinct().show(truncate=False)\n",
    "    \n",
    "    # List, if any, the alternate titles for the movie.\n",
    "    print(f\"\\nAlternate titles for '{best_movie_title}':\")\n",
    "    df_alternate_titles = df_akas.filter(col(\"tconst\") == lit(best_movie_tconst)) \\\n",
    "                                 .select(\"title\", \"region\", \"language\") \\\n",
    "                                 .orderBy(\"region\", \"language\")\n",
    "    \n",
    "    df_alternate_titles.show(truncate=False)\n",
    "    \n",
    "except IndexError:\n",
    "    print(f\"ANALYTICAL ERROR: No comedy movies found with at least {MIN_VOTES_THRESHOLD} votes. Try lowering the threshold.\")\n",
    "except AnalysisException as e:\n",
    "     print(f\" ANALYSIS ERROR (Check Join/Column Names): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053037a5-ca0d-44af-86ab-90b3f7eed755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# --- CELL 4: STRUCTURED STREAMING SIMULATION ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5ca2a9-cd98-4329-9113-7cdd8d709c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import window, rand, array, size, lit, current_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# 1. Define Checkpoint Paths and UC Table Names\n",
    "\n",
    "UC_VOLUME_NAME_EXPLICIT = \"raw_imdb_files\" \n",
    "\n",
    "\n",
    "CHECKPOINT_ROOT = f\"dbfs:/Volumes/workspace/imdb_project/{UC_VOLUME_NAME_EXPLICIT}/\"\n",
    "\n",
    "CHECKPOINT_PATH_METRICS = CHECKPOINT_ROOT + \"stream_checkpoints_metrics_final/cp\" \n",
    "CHECKPOINT_PATH_ALERTS = CHECKPOINT_ROOT + \"stream_checkpoints_alerts_final/cp\"   \n",
    "\n",
    "\n",
    "UC_TABLE_METRICS = \"workspace.imdb_project.stream_metrics_output\"\n",
    "UC_TABLE_ALERTS = \"workspace.imdb_project.stream_alerts_output\"\n",
    "\n",
    "# Define target entities (using results from analysis or placeholders)\n",
    "try:\n",
    "    # Retrieve variables from Cell 3 if they exist\n",
    "    TRACK_TCONST = best_movie_tconst\n",
    "    TRACK_NCONST = df_directors.select(\"nconst\").limit(1).collect()[0][\"nconst\"]\n",
    "except:\n",
    "    # Placeholders if batch analysis failed or wasn't run\n",
    "    TRACK_TCONST = \"tt0252487\" \n",
    "    TRACK_NCONST = \"nm0250608\"\n",
    "\n",
    "TRACK_ENTITIES = [TRACK_TCONST, TRACK_NCONST, \"Genre_Action\", \"User_Type_Bot\"]\n",
    "\n",
    "\n",
    "# 2. Define the Simulated Stream DataFrame (Source: using 'rate' source)\n",
    "df_stream_raw = spark.readStream.format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 5) \\\n",
    "    .load() \\\n",
    "    .withColumnRenamed(\"timestamp\", \"event_time\") \\\n",
    "    .withColumn(\"tconst\", F.array(*[lit(e) for e in TRACK_ENTITIES])[F.rand().cast(IntegerType()) % F.size(F.array(*[lit(e) for e in TRACK_ENTITIES]))]) \\\n",
    "    .withColumn(\"byte_change\", (F.rand() * 5000).cast(IntegerType())) \\\n",
    "    .withColumn(\"is_bot\", (F.rand() > 0.8))\n",
    "\n",
    "# Apply Watermark for time-based aggregation\n",
    "df_stream_watermarked = df_stream_raw.withWatermark(\"event_time\", \"5 minutes\")\n",
    "\n",
    "\n",
    "# METRIC: Edits per Entity in 2-minute Tumbling Window \n",
    "df_metrics = df_stream_watermarked.groupBy(\n",
    "    F.window(col(\"event_time\"), \"2 minutes\"), \n",
    "    col(\"tconst\")\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"edit_count\"),\n",
    "    F.sum(\"byte_change\").alias(\"total_bytes\")\n",
    ")\n",
    "\n",
    "# ALERT: Trigger for specific user type (non-bot) making a massive change \n",
    "ALERT_THRESHOLD_BYTES = 1000\n",
    "ALERT_TARGET_TCONST = lit(TRACK_TCONST)\n",
    "\n",
    "df_alerts = df_stream_watermarked.filter(\n",
    "    (col(\"tconst\") == ALERT_TARGET_TCONST) & \n",
    "    (col(\"byte_change\") >= ALERT_THRESHOLD_BYTES) & \n",
    "    (col(\"is_bot\") == lit(False))\n",
    ").select(\n",
    "    col(\"event_time\"), \n",
    "    col(\"tconst\").alias(\"entity_id\"), \n",
    "    col(\"byte_change\"), \n",
    "    lit(\"LARGE_NON_BOT_EDIT\").alias(\"alert_type\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Define Output Sinks (Using AvailableNow trigger to write to UC Tables)\n",
    "\n",
    "print(f\"\\nStarting Metrics Stream (Complete Mode) to UC Table: {UC_TABLE_METRICS}...\")\n",
    "\n",
    "# Explicit construction of the metrics query\n",
    "query_metrics_writer = df_metrics.writeStream\n",
    "query_metrics_writer = query_metrics_writer.format(\"delta\")\n",
    "query_metrics_writer = query_metrics_writer.outputMode(\"complete\") \n",
    "query_metrics_writer = query_metrics_writer.option(\"checkpointLocation\", CHECKPOINT_PATH_METRICS)\n",
    "query_metrics_writer = query_metrics_writer.queryName(\"Metrics_Query\")\n",
    "query_metrics_writer = query_metrics_writer.trigger(availableNow=True)\n",
    "query_metrics = query_metrics_writer.toTable(UC_TABLE_METRICS) \n",
    "\n",
    "\n",
    "print(f\"Starting Alerts Stream (Append Mode) to UC Table: {UC_TABLE_ALERTS}...\")\n",
    "\n",
    "# Explicit construction of the alerts query\n",
    "query_alerts_writer = df_alerts.writeStream\n",
    "query_alerts_writer = query_alerts_writer.format(\"delta\")\n",
    "query_alerts_writer = query_alerts_writer.outputMode(\"append\")\n",
    "query_alerts_writer = query_alerts_writer.option(\"checkpointLocation\", CHECKPOINT_PATH_ALERTS)\n",
    "query_alerts_writer = query_alerts_writer.queryName(\"Alerts_Query\")\n",
    "query_alerts_writer = query_alerts_writer.trigger(availableNow=True)\n",
    "query_alerts = query_alerts_writer.toTable(UC_TABLE_ALERTS)\n",
    "\n",
    "\n",
    "print(\"\\nStreaming queries started. Check Databricks UI for status.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1a9d74-023f-4881-a087-65b9d031781c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# --- STREAMING VERIFICATION CELL (READING FROM UC TABLES) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ef1106-cb0a-45c8-bebf-89eae1ea58c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Define the Managed Unity Catalog Table NAMES (Must match those in Cell 4)\n",
    "UC_TABLE_METRICS = \"workspace.imdb_project.stream_metrics_output\"\n",
    "UC_TABLE_ALERTS = \"workspace.imdb_project.stream_alerts_output\"\n",
    "\n",
    "print(\"--- VERIFYING AGGREGATED METRICS (UC TABLE) ---\")\n",
    "try:\n",
    "    # Read directly from the UC table by name\n",
    "    df_metrics_result = spark.table(UC_TABLE_METRICS)\n",
    "    print(f\"Total processed metric windows: {df_metrics_result.count():,}\")\n",
    "    \n",
    "    # Display the top 10 rows, sorted by edit count\n",
    "    df_metrics_result.orderBy(F.col(\"edit_count\").desc()).show(10, truncate=False)\n",
    "    \n",
    "except AnalysisException as e:\n",
    "    print(f\"❌ METRICS READING FAILED: The table '{UC_TABLE_METRICS}' might not exist yet, or permissions are missing.\")\n",
    "    print(f\"Error: {e.desc}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ METRICS READING FAILED. Error: {e}\")\n",
    "    \n",
    "print(\"\\n--- VERIFYING TRIGGERED ALERTS (UC TABLE) ---\")\n",
    "try:\n",
    "    # Read directly from the UC table by name\n",
    "    df_alerts_result = spark.table(UC_TABLE_ALERTS)\n",
    "    print(f\"Total triggered alerts: {df_alerts_result.count():,}\")\n",
    "    \n",
    "    # Display the alerts\n",
    "    df_alerts_result.show(10, truncate=False)\n",
    "    \n",
    "except AnalysisException as e:\n",
    "    print(f\"❌ ALERTS READING FAILED: The table '{UC_TABLE_ALERTS}' might not exist yet, or permissions are missing.\")\n",
    "    print(f\"Error: {e.desc}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ALERTS READING FAILED. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "project_BD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
